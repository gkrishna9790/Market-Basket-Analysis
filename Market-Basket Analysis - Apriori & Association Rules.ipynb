{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Market Basket Analysis\n",
    "\n",
    "This analysis uses 'Online Retail' data from UCI data collection, and is aimed provide a background on **Apriori algorithm**, and a practical implementation of the same via Python.  \n",
    "\n",
    "## Primer on Apriori Algorithm & Association Rules\n",
    "  \n",
    "Apriori algorithms is a data mining algorithm used for mining **frequent itemsets** and **relevant association rules**. It is devised to operate on a database that contain transactions -like, items bought by a customer in a store. \n",
    "\n",
    "An itemset can be considered ***frequent*** if it meets a user-specified support threshold. For example, if the support threshold is set to 0.5(50%), a frequent itemset is a set of items that are bought/purchased together in atleast 50% of all transactions. \n",
    "\n",
    "***Association rules*** are a set of rules derived from a database, that can help determining relationship among variables in a large transactional database. \n",
    "\n",
    "For example, let I ={i(1),i(2)...,i(m)} be a set of m attributes called items, and T={t(1),t(2),...,t(n)} be the set of transactions. Every transaction t(i) in T has a unique transaction ID, and it contains a subset of itemsets in I.\n",
    "\n",
    "Association rules are usually written as **i(j) -> i(k)**. This means that there is a strong relationship between the purchase of item i(j) and item i(k). Both these items were purchased together in the same transaction. \n",
    "  \n",
    "In the above example, i(j) is the **antecedent** and i(k) is the **consequent**. \n",
    "\n",
    "Please note that both antecedents and consequents can have multiple items. For example, {Diaper,Gum} -> {Beer, Chips} is also valid. \n",
    "\n",
    "Since multiplie rules are possible even from a very small database, i-order to select the most relevant ones, we use constraints on various measures of interest. The most important measures are discussed below. They are:\n",
    "\n",
    "** 1. Support : ** The support of an itemset X, *supp(X)* is the proportion of transaction in the database in which the item X appears. It signifies the popularity of an itemset.\n",
    "\n",
    "supp(X) = (Number of transactions in which X appears)/(Total number of transactions)\n",
    "  \n",
    "We can identify itemsets that have support values beyond this threshold as significant itemsets.  \n",
    "\n",
    "** 2. Confidence :** Confidence of a rule signifies the likelihood of item Y being purchased when item X is purchased. \n",
    "\n",
    "Thus, *conf(X -> Y) = supp(X *U* Y) / supp( X )*\n",
    "\n",
    "If conf (X -> Y) is 75%, it implies that, for 75% of transactions containing X & Y, this rule is correct. It is more like a conditional probability, P(Y|X), that the probability of finding itemset Y in transactions fiven that the transaction already contains itemset X.\n",
    "  \n",
    "  \n",
    "** 3. Lift :** Lift explains the the likelihood of the itemset Y being purchased when itemset X is already purchased, while taking into account the popularity of Y. \n",
    "  \n",
    "Thus, *lift (X -> Y) = supp (X *U* Y)/( supp(X) * supp (Y) )*\n",
    "\n",
    "If the value of lift is greater than 1, it means that the itemset Y is likely to be bought with itemset X, while a value less than 1 implies that the itemset Y is unlikely to be bought if the itemset X is bought. \n",
    "\n",
    "** 4. Conviction :** The conviction of a rule can be defined as :\n",
    "\n",
    "*conv (X->Y) = (1-supp(Y))/(1-conf(X-Y))*\n",
    "\n",
    "If the conviction means 1.4, it means that the rule X -> Y would be incorrect 40% more often if the association between X & Y was an accidental chance.\n",
    "\n",
    "### Steps in Apriori Algorithm\n",
    "\n",
    "The steps in implementing Apriori Algorithm are:\n",
    "  \n",
    "1. Create a frequency table of all items that occur in all transactions.\n",
    "  \n",
    "2. Select only those (significant) items - for which the support is greater than threshold (50%)\n",
    "  \n",
    "3. Create possible pairs of all items (remember AB is same as BA)\n",
    "  \n",
    "4. Select itemsets that are only significant (support > threshold)\n",
    "\n",
    "5. Create tiplets using another rule, called self-join. It says, from the item pairs AB, AC, BC, BD, we look for pairs with identical first letter. So we from AB, AC we get ABC. From BC, BD we get BCD.\n",
    "  \n",
    "6. Find frequency of the new triplet pairs, and select only those pairs where the support of the new itemset (ABC or BCD) is greater than the threshold.  \n",
    "  \n",
    "7. If we get 2 pairs of significant triplets, combine and form groups of 4, repeat the threshold process, and continue.\n",
    "  \n",
    "8. Continue till the frequency after grouping is less than threshold support. \n",
    "\n",
    "### Pros of Apriori algorithm:\n",
    "\n",
    "1. Easy to understand and implement\n",
    "2. Can be used on large itemsets\n",
    "\n",
    "### Cons of Apriori algoritm\n",
    "\n",
    "1. Can get compuationally expensive if the candidate rules are large\n",
    "2. Calculating support is also expensive since it has to go through the whole database\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
